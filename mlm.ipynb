{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 31 10:06:31 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100 80G...  Off  | 00000000:CA:00.0 Off |                   On |\n",
      "| N/A   35C    P0    62W / 300W |                  N/A |     N/A      Default |\n",
      "|                               |                      |              Enabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                |\n",
      "+------------------+----------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n",
      "|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\n",
      "|                  |                      |        ECC|                       |\n",
      "|==================+======================+===========+=======================|\n",
      "|  0    2   0   0  |  16339MiB / 40192MiB | 42      0 |  3   0    2    0    0 |\n",
      "|                  |      4MiB / 65535MiB |           |                       |\n",
      "+------------------+----------------------+-----------+-----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForWholeWordMask\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import default_data_collator\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"DISABLE_MLFLOW_INTEGRATION\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(os.getenv('TOKENIZER_PATH'))\n",
    "dataset = load_dataset('parquet', data_files={'train': 'df_train.parquet'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем для каждого токена его word_id\n",
    "\n",
    "def mlm_tokenize_function(examples):\n",
    "    result = tokenizer(examples['description'])\n",
    "    if tokenizer.is_fast:\n",
    "        result['word_ids'] = [result.word_ids(i) for i in range(len(result['input_ids']))]\n",
    "    return result\n",
    "\n",
    "mlm_tokenized_datasets = dataset['train'].map(\n",
    "    mlm_tokenize_function, batched=True, num_proc=8\n",
    ").select_columns(\n",
    "    ['attention_mask', 'input_ids', 'word_ids']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Группируем тексты в чанки по 256 токенов, чтобы избежать лишних вычислений на PAD токенах\n",
    "\n",
    "def group_texts(examples, chunk_size=256):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "mlm_tokenized_datasets = mlm_tokenized_datasets.map(group_texts, batched=True, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Маскируем случайные 15% слов\n",
    "\n",
    "wwm_probability = 0.15\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature[\"word_ids\"]\n",
    "\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return default_data_collator([{k: v for k, v in feature.items() if k != 'word_ids'} for feature in features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разбиение на тренировку и валидацию\n",
    "\n",
    "mlm_tokenized_datasets = mlm_tokenized_datasets.train_test_split(\n",
    "    train_size=int(len(mlm_tokenized_datasets) * 0.9), test_size=int(len(mlm_tokenized_datasets) * 0.1), seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тренировка на MLM\n",
    "mlm_model = AutoModelForMaskedLM.from_pretrained(os.getenv('MODEL_PATH'))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=mlm_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=os.getenv('OUTPUT_DIR'),\n",
    "        num_train_epochs=1, warmup_steps=100,\n",
    "        optim='adamw_torch', learning_rate=1e-4, weight_decay=1e-2,\n",
    "        fp16=True, max_grad_norm=1.0, gradient_accumulation_steps=4,\n",
    "        per_device_train_batch_size=16, per_device_eval_batch_size=64,\n",
    "        do_eval=True, eval_strategy='steps', eval_steps=100,\n",
    "        remove_unused_columns=False,\n",
    "    ),\n",
    "    train_dataset=mlm_tokenized_datasets['train'],\n",
    "    eval_dataset=mlm_tokenized_datasets['test'],\n",
    "    data_collator=whole_word_masking_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "main_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
