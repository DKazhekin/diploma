{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9912f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import mle_tools\n",
    "import pyspark\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import re\n",
    "import ast\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import pyspark.sql.functions as F\n",
    "from mle_tools.data_minings import connections, formater\n",
    "from pyspark.sql import Window as W\n",
    "from joblib import delayed, Parallel\n",
    "from pandas import Timestamp\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f0f68",
   "metadata": {},
   "source": [
    "# Подготовить спарк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fbe389",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = connections.get_spark({\n",
    "    'spark.app.name': \"data_collecting\",\n",
    "    'spark.executor.memory': '32g',\n",
    "    'spark.driver.memory': '32g',\n",
    "    'spark.dynamicAllocation.maxExecutors': '32',\n",
    "    'spark.sql.execution.arrow.pyspark.enabled': \"true\",\n",
    "})\n",
    "\n",
    "fs = connections.prepare_env_for_pyarrow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f139ea5-90b6-416d-bd7a-13f92f36fbe8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Разметка #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d541eb04-3830-4789-8aab-f88344c00202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.getenv('SCENARIO_PATH'), \"r\", encoding=\"utf-8\") as filepath:\n",
    "    table = json.load(filepath)\n",
    "    \n",
    "scenario = pd.DataFrame(table['info'])\n",
    "scenario_sdf = spark.createDataFrame(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2fe3cb-66c3-4477-9a1f-a6580f717406",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Добавить инфо к разметке #1 + разметка #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeb2742-2607-4dd3-9f5e-3eb1398971cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logs = (\n",
    "    spark.table(os.getenv('LOGS'))\n",
    "    \n",
    "    .filter(F.col('date_part') >= '20250201')\n",
    "    \n",
    "    .withColumn('rk', F.row_number().over(W.partitionBy('id').orderBy(F.desc('time'))))\n",
    "    .filter(F.col('rk') == 1)\n",
    "    .drop('rk') \n",
    "    \n",
    "    .select('id', 'time', 's_id', 'channel', 'text', 'client_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c195844-3b33-44b2-aea6-6a767dfd443d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "selected = logs.join(scenario_sdf, scenario_sdf.text==logs.text).select('text', 'id', 's_id', 'markup')\n",
    "\n",
    "old_data = formater.spark_df_to_pandas(selected, fs).rename(columns={'s_id': 'session_id'})\n",
    "new_data = formater.spark_df_to_pandas(spark.sql(f\"SELECT * FROM {os.getenv('LABELS_PATH')}\"), fs)[['text', 'id', 'session_id', 'markup']]\n",
    "\n",
    "data_df = pd.concat((old_data, new_data))\n",
    "data = spark.createDataFrame(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732888fc",
   "metadata": {},
   "source": [
    "# Сбор данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c86f65-6589-4117-85fe-7089fd708ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_selected = data.select('session_id').dropDuplicates(['session_id']) \n",
    "sessions_sdf = logs.join(data_selected, logs.s_id==data_selected.session_id)\n",
    "sessions_pdf = formater.spark_df_to_pandas(sessions_sdf, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8714b14-79a6-4f10-9a2e-0442be1f4494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s2id = {\n",
    "    sid: cid\n",
    "    for sid, cid in zip(sessions_pdf['s_id'], sessions_pdf['client_id'])\n",
    "    if cid is not None\n",
    "       and cid != 'Bot'\n",
    "       and '_' not in cid\n",
    "}\n",
    "\n",
    "sessions_pdf['client_id'] = sessions_pdf['s_id'].map(lambda sid: s2id.get(sid, sid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d99af82-5092-4c6e-9e8f-1f4c9e9bfb2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client_ids = spark.createDataFrame(sessions_pdf.drop_duplicates(subset=['client_id']))\n",
    "clients = logs.join(client_ids, logs.client_id==client_ids.client_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba88708b-ab4d-428f-b7ca-e8b58d33bee7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "session_ids = (\n",
    "    clients\n",
    "    .select(F.col('s_id').alias('session_id'))\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "client_sessions_df = formater.spark_df_to_pandas(\n",
    "    logs.join(session_ids, logs.s_id == session_ids.session_id),\n",
    "    fs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5beb3f-ca87-44f6-915a-d2d866c9db61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s2id = {\n",
    "    sid: cid\n",
    "    for sid, cid in zip(client_sessions_df['s_id'], client_sessions_df['client_id'])\n",
    "    if cid is not None\n",
    "       and cid != 'AlfaBot'\n",
    "       and '_' not in cid\n",
    "}\n",
    "\n",
    "client_sessions_df['client_id'] = client_sessions_df['s_id'].map(lambda sid: s2id.get(sid, sid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c877e4f-e425-442f-a0a8-3aea614b1504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "formatter = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "\n",
    "client_sessions_df = client_sessions_df.sort_values('time')\n",
    "client_sessions_df = client_sessions_df[~client_sessions_df['time'].isna()]\n",
    "client_sessions_df['time'] = client_sessions_df['time'].apply(lambda x: datetime.strptime(x, formatter))\n",
    "client_sessions_df = client_sessions_df.groupby('client_id').agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deff8f3-f6a3-4b63-afe4-073d45c8a63a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_blocks = 1000\n",
    "block_size = len(client_sessions_df) // n_blocks\n",
    "for i in range(n_blocks):\n",
    "    client_sessions_df[i * block_size: (i + 1) * block_size].to_csv(f'{os.getenv('LOCAL_PATH')}/chunk{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b051a-b804-4264-98ee-9dc724199a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 120/1000 [00:31<05:07,  2.86it/s]/home/jovyan/envs/main_env/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:756: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  UserWarning,\n",
      "100%|██████████| 1000/1000 [13:06<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "def trim_substrings(texts, substring='version 0:'):\n",
    "\n",
    "    def _trim(text):\n",
    "        if not text:\n",
    "            return ''\n",
    "        matches = list(re.finditer(re.escape(substring), text))\n",
    "        if len(matches) == 2:\n",
    "            return text[matches[0].end():matches[1].start()]\n",
    "        return text\n",
    "\n",
    "    return [_trim(t) for t in texts]\n",
    "\n",
    "def parse_timestamps(ts_str):\n",
    "\n",
    "    pattern = re.compile(r\"Timestamp\\('([^']+)'\\)\")\n",
    "    return [Timestamp(ts) for ts in pattern.findall(ts_str)]\n",
    "\n",
    "def format_context(texts, channels):\n",
    "    \n",
    "    prev = channels[0]\n",
    "    ctx = '[BOT]' if prev == 'AM' else '[CLIENT]'\n",
    "\n",
    "    for text, ch in zip(texts, channels):\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        if ch != prev:\n",
    "            ctx += ' ' + ('[BOT]' if ch == 'AM' else '[CLIENT]') + ' '\n",
    "            prev = ch\n",
    "        else:\n",
    "            ctx += ' '\n",
    "\n",
    "        ctx += re.sub(r\"\\[(.*?)\\]\\(.*?\\)\", r\"\\1\", text)\n",
    "\n",
    "    return ctx\n",
    "\n",
    "def process_file(path):\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        converters={\n",
    "            'id':         ast.literal_eval,\n",
    "            'time':       parse_timestamps,\n",
    "            'channel':    ast.literal_eval,\n",
    "            'text':       ast.literal_eval\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df['text'] = df['text'].apply(trim_substrings)\n",
    "\n",
    "    records = []\n",
    "    \n",
    "    for ids, times, channels, texts in zip(df['id'], df['time'], df['channel'], df['text']):\n",
    "        start = 0\n",
    "        for i, (msg_id, t, ch) in enumerate(zip(ids, times, channels)):\n",
    "            while (times[i] - times[start]).days >= 7 or i - start > 10:\n",
    "                start += 1\n",
    "\n",
    "            ctx = format_context(texts[start:i+1], channels[start:i+1])\n",
    "            if ctx:\n",
    "                records.append({\n",
    "                    'id':   msg_id,\n",
    "                    'cntxt': ctx.replace('\\n', ' ').replace('\\r', ' ')\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "directory = os.getenv('UPLOAD_PATH')\n",
    "csv_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "dfs = Parallel(n_jobs=-1)(\n",
    "    delayed(process_file)(file) for file in tqdm(csv_files)\n",
    ")\n",
    "\n",
    "result_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6fa233-3367-4252-8b1e-9282137eab26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "markuped = pd.merge(dfs, data_df, on='id', how='right')[['cntxt', 'markup']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ca1bc-98bb-4f61-b2f1-7e81654674c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_val, test = train_test_split(markuped, test_size=0.15, stratify=markuped['intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a9338",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train_val, test_size=0.1, stratify=train_val['intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d740dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(os.getenv('LOCAL_TRAIN_PATH'))\n",
    "val.to_csv(os.getenv('LOCAL_VAL_PATH'))\n",
    "test.to_csv(os.getenv('LOCAL_TEST_PATH'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "main_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
